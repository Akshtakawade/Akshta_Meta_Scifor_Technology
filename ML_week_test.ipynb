{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPiOiVh2Fx0IPly789U6cYu"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "twx3ht9ZPbXx"
      },
      "outputs": [],
      "source": [
        "#1.How does regularization (L1 and L2) help in preventing overfitting?\n",
        "L1 and L2 regularization are techniques used in machine learning to prevent overfitting.\n",
        "Overfitting is a phenomenon where the model learns the noise in the data instead of the underlying patterns, which leads to poor performance on new, unseen data.\n",
        "\n",
        "L1 regularization (Lasso) adds a penalty to the model based on the **absolute value** of the weights. This encourages the model to ignore less important features,\n",
        "effectively selecting only the most relevant ones. As a result, the model becomes simpler and less likely to overfit.\n",
        "\n",
        "L2 regularization (Ridge) adds a penalty based on the **square** of the weights.\n",
        "This encourages the model to spread the importance evenly across all features, preventing it from relying too much on any one feature and helping reduce overfitting.\n",
        "\n",
        "L1 and L2 regularization can prevent overfitting by reducing the complexity of the model and distributing the weights more evenly across all the features."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#2.Why is feature scaling important in gradient descent?\n",
        "Feature scaling is important in gradient descent because it helps the algorithm learn faster and more efficiently by ensuring all features are on the same scale.\n",
        "\n",
        "Faster Convergence: Features with different scales can cause uneven updates, slowing down the learning. Scaling makes the updates more consistent and speeds up convergence.\n",
        "\n",
        "Prevents Skewed Updates: Without scaling, features with larger values can dominate the learning process, making updates unstable. Scaling ensures each feature has a similar impact.\n",
        "\n",
        "Improves Optimization: Scaling helps the algorithm find the best solution more easily by avoiding issues caused by features with very different magnitudes.\n",
        "\n",
        "Min-Max Scaling: Rescales features to a range (e.g., 0 to 1).\n",
        "Standardization: Centers features around 0 with a standard deviation of 1.\n",
        "In short, feature scaling helps gradient descent work more effectively and quickly by treating all features equally."
      ],
      "metadata": {
        "id": "SLkIMsI8SCpu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Problem Solving\n",
        "#1.Given a dataset with missing values, how would you handle them before training an ML model?\n",
        "\n",
        "If the missing data is small - Dropping or imputing with the mean/median can work.\n",
        "If the missing data is substantial - Imputation with advanced methods (regression) or using decision tree models that handle missing data natively might be better.\n",
        "If missing data is not random - Consider treating the missing values as a separate category or using a predictive model for imputation.\n",
        "the method to handle missing data depends on the amount of missing data, the type of data (numerical or categorical), and the algorithm you're using."
      ],
      "metadata": {
        "id": "1J4Y6XoiS7SW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#2 Design a pipeline for building a classification model. Include steps for data preprocessing.\n",
        "\n",
        "1 Import needed libraries\n",
        "2 Load the data set - data = pd.read_csv('your_dataset.csv')\n",
        "3 Data Preprocessing - data cleaning, Handling missing values Fill missing values with mean or mode etc\n",
        "4 Split the data into training and testing sets\n",
        "5 Train the model using a classification algorithm (e.g., Random Forest)\n",
        "6 Check the models accuracy on the test data\n",
        "This pipeline covers the essential steps to clean, train, evaluate, and save a classification model in a simple and efficient way."
      ],
      "metadata": {
        "id": "DyKM8xYpT1NR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Coding\n",
        "#1.Write a Python script to implement a decision tree classifier using Scikit-learn.\n",
        "\n",
        "# Import necessary libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "\n",
        "# Load dataset (for demonstration, we will use the Iris dataset)\n",
        "from sklearn.datasets import load_iris\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create a Decision Tree Classifier\n",
        "clf = DecisionTreeClassifier(random_state=42)\n",
        "\n",
        "# Fit the model on the training data\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test data\n",
        "y_pred = clf.predict(X_test)\n",
        "\n",
        "# Evaluate the model\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "conf_matrix = confusion_matrix(y_test, y_pred)\n",
        "class_report = classification_report(y_test, y_pred)\n",
        "\n",
        "# Print the results\n",
        "print(f\"Accuracy: {accuracy:.2f}\")\n",
        "print(\"Confusion Matrix:\\n\", conf_matrix)\n",
        "print(\"Classification Report:\\n\", class_report)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "peNOXTVKVybL",
        "outputId": "fee462f1-e1a4-42ee-868d-bf69af792fc2"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 1.00\n",
            "Confusion Matrix:\n",
            " [[10  0  0]\n",
            " [ 0  9  0]\n",
            " [ 0  0 11]]\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00        10\n",
            "           1       1.00      1.00      1.00         9\n",
            "           2       1.00      1.00      1.00        11\n",
            "\n",
            "    accuracy                           1.00        30\n",
            "   macro avg       1.00      1.00      1.00        30\n",
            "weighted avg       1.00      1.00      1.00        30\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#2 Given a dataset, write code to split the data into training and testing sets using an 80-20 split.\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Sample dataset creation\n",
        "data = {\n",
        "    'feature1': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10],\n",
        "    'feature2': [10, 9, 8, 7, 6, 5, 4, 3, 2, 1],\n",
        "    'label': [0, 1, 0, 1, 0, 1, 0, 1, 0, 1]\n",
        "}\n",
        "\n",
        "# Creating a DataFrame\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Splitting the dataset into training and testing sets\n",
        "train_set, test_set = train_test_split(df, test_size=0.2, random_state=42)\n",
        "\n",
        "# Displaying the results\n",
        "print(\"Training Set:\")\n",
        "print(train_set)\n",
        "print(\"\\nTesting Set:\")\n",
        "print(test_set)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bOPkwwZ2dipT",
        "outputId": "1f961eac-6434-41fc-d9e5-8e642e31210b"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Set:\n",
            "   feature1  feature2  label\n",
            "5         6         5      1\n",
            "0         1        10      0\n",
            "7         8         3      1\n",
            "2         3         8      0\n",
            "9        10         1      1\n",
            "4         5         6      0\n",
            "3         4         7      1\n",
            "6         7         4      0\n",
            "\n",
            "Testing Set:\n",
            "   feature1  feature2  label\n",
            "8         9         2      0\n",
            "1         2         9      1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Case Study\n",
        "# A company wants to predict employee attrition. What kind of ML problem is this? Which algorithms would you choose and why?\n",
        "\n",
        "This is a binary classification problem. The objective is to predict whether an employee will leave the company (attrition = 1) or stay (attrition = 0)\n",
        "based on various features (e.g., age, salary, department, job satisfaction, years at the company).\n",
        "\n",
        "For predicting employee attrition, Logistic Regression is a good starting point because it's simple and interpretable. If you want better performance, try Decision Trees."
      ],
      "metadata": {
        "id": "wEADTB5leTRI"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}